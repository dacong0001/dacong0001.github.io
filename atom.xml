<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大葱的笔记</title>
  
  <subtitle>大数据</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-03-12T02:26:31.605Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Dacong001</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SQL窗口函数</title>
    <link href="http://example.com/2022/03/12/SQL%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"/>
    <id>http://example.com/2022/03/12/SQL%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</id>
    <published>2022-03-12T02:25:36.000Z</published>
    <updated>2022-03-12T02:26:31.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="窗口函数："><a href="#窗口函数：" class="headerlink" title="窗口函数："></a>窗口函数：</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;窗口函数：&quot;&gt;&lt;a href=&quot;#窗口函数：&quot; class=&quot;headerlink&quot; title=&quot;窗口函数：&quot;&gt;&lt;/a&gt;窗口函数：&lt;/h1&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Sql高级</title>
    <link href="http://example.com/2022/03/11/Sql%E9%AB%98%E7%BA%A7/"/>
    <id>http://example.com/2022/03/11/Sql%E9%AB%98%E7%BA%A7/</id>
    <published>2022-03-11T04:56:53.000Z</published>
    <updated>2022-03-12T09:56:15.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>索引：<font color='red'>排好序的快速查找数据结构</font></p><h2 id="BTree索引："><a href="#BTree索引：" class="headerlink" title="BTree索引："></a>BTree索引：</h2><p>（多路搜索树，不一定是二叉树）</p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646643638713.png" class="" width="1646643638713"><p>​    每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样可以运用二分查找在一定的复杂度内获取到相应数据，从而    快速的检索出符合条件的记录。</p><p>索引的劣势：虽然提高了速度，但是降低了表更新的速度，例如INSTER 、UPDATE 、DELETE。如果有大量的表，就要花时间研究最优秀的索引</p><h3 id="索引分类"><a href="#索引分类" class="headerlink" title="索引分类"></a>索引分类</h3><p><strong>单值索引：</strong>一个索引只包含单个列，一个表可以有多个单值索引。</p><p><strong>唯一索引：</strong>索引列的值必须唯一，但允许空值。</p><p><strong>复合索引：</strong>一个索引有多个列。</p><p><strong>基本语法：</strong></p><p>创建：</p><p><code>CREAT [UNIQUE] INDEX 索引名 ON 表名(字段（长度）);</code></p><p><code>ALTER 表名 ADD [UNIQUE] INDEX [索引名] ON (字段（长度）);</code></p><p>删除：</p><p><code>DROP INDEX [索引名] ON 表名;</code></p><p>查看：</p><p><code>SHOW INDEX FROM 表名\G</code></p><h3 id="索引结构"><a href="#索引结构" class="headerlink" title="索引结构"></a>索引结构</h3><p><strong>那些情况需要建立索引：</strong></p><ol><li><p>主键自动创建</p></li><li><p>频繁作为查找条件的字段</p></li><li><p>查询中与其他表关联的字段</p></li><li><p>频繁更新的字段<font color='red'>不适用</font></p></li><li><p>Where条件里用不到的字段<font color='red'>不适用</font></p></li><li><p>单键&#x2F;组合索引选择（高并发下创建组合索引）</p></li><li><p>查询排序的字段，排序字段若通过索引会大大提高素速度</p></li><li><p>查询中统计或分组的字段</p></li></ol><p><strong>那些不要建立索引：</strong></p><ol><li>表记录太少</li><li>经常增删改的表</li><li>数据重复且平均分布的字段，没有太大的实际效果（男女，国籍，等等）</li></ol><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646647186242.png" class="" width="1646647186242"><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646647196566.png" class="" width="1646647196566"><h3 id="B-Tree与B-Tree-的区别"><a href="#B-Tree与B-Tree-的区别" class="headerlink" title="B+Tree与B-Tree 的区别"></a>B+Tree与B-Tree 的区别</h3><p>　1）B-树的<font color='red'>关键字和记录是放在一起的</font>，叶子节点可以看作外部节点，不包含任何信息；B+树的非叶子节点中只有关键字和指向下一个节点的索引，<font color='red'>记录只放在叶子节点中</font>。<br>　 2）在B-树中，<font color='red'>越靠近根节点的记录查找时间越快</font>，只要找到关键字即可确定记录的存在；而B+树中每个记录的<font color='red'>查找时间基本是一样的</font>，都需要从根节点走到叶子节点，而且在叶子节点中还要再比较关键字。从这个角度看B-树的性能好像要比B+树好，而在实际应用中却是B+树的性能要好些。因为B+树的非叶子节点不存放实际的数据，这样每个节点可容纳的<font color='red'>元素个数比B-树多，树高比B-树小</font>，这样带来的好处是减少磁盘访问次数。尽管B+树找到一个记录所需的比较次数要比B-树多，但是一次磁盘访问的时间相当于成百上千次内存比较的时间，因此实际中B+树的性能可能还会好些，而且B+树的叶子节点使用指针连接在一起，方便顺序遍历（例如查看一个目录下的所有文件，一个表中的所有记录等），这也是很多数据库和文件系统使用B+树的缘故。<br>　<br>思考：为什么说B+树比B-树更适合实际应用中操作系统的文件索引和数据库索引？<br><strong>1) B+树的磁盘读写代价更低</strong><br>　　B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。<br><strong>2) B+树的查询效率更加稳定</strong><br>　　由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</p><h3 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h3><p>Mysq常见瓶颈</p><blockquote><p>CPU：数据装入或读取</p><p>IO：装入数据远大于内存</p><p>服务器：用  top、free、等命令查看系统性能</p></blockquote><h2 id="Explain"><a href="#Explain" class="headerlink" title="Explain"></a>Explain</h2><p>​    使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。</p><p><strong>用法：</strong>  <code>explain select * from emp;</code>    explain + Sql 语句</p><h3 id="能干嘛："><a href="#能干嘛：" class="headerlink" title="能干嘛："></a>能干嘛：</h3><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646646266255.png" class="" width="1646646266255"><p><strong>各字段解释：</strong></p><ul><li><p><strong>id：</strong>select 查询的序列号，包含一组数字，查询select字句或操作表的顺序</p><blockquote><p>Id相同的情况下，执行顺序为从上到下</p><p>Id不同，按照id顺序执行id值越大，越先执行。</p><p>Id相同不同 ，同时存在：id如果相同，可以认为是一组，从上往下顺序执行;在所有组中，id值越大，优先级越高，越先执行</p></blockquote></li><li><p><strong>select_type:</strong></p><blockquote><p>**SIMPLE:**简单的查询，不包含子查询或UNION</p><p><strong>PRIMARY：</strong> 查询中包含子查询</p><p><strong>SUBQUERY：</strong>在select或where中包含子查询</p><p><strong>DERIVED：</strong>在FROM列表中包含的子查询被标记为DERIVED(衍生)MySQL会递归执行这些子查询, 把结果放在临时表里。</p><p><strong>UNION：</strong>若第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中,外层SELECT将被标记为：DERIVED</p><p><strong>UNION RESULT：</strong>从UNION表获取结果的SELECT</p></blockquote></li><li><p><strong>type：</strong></p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646650823623.png" class="" width="1646650823623"><p><font color='red'>从最好到最差依次是：</font><br>system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;ALL</p><blockquote><p><strong>system：</strong>表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计</p><p><strong>const：</strong>表示通过索引一次就找到了,const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快如将主键置于where列表中，MySQL就能将该查询转换为一个常量</p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646651068930.png" class="" width="1646651068930"><p><strong>eq_ref：</strong>唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描</p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646650906856.png" class="" width="1646650906856"><p>**ref:**非唯一性索引扫描，返回匹配某个单独值的所有行.本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体.</p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646650941814.png" class="" width="1646650941814"><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646651027958.png" class="" width="1646651027958"><p>**<font color='orange'>range</font>:**只检索给定范围的行,使用一个索引来选择行。key 列显示使用了哪个索引一般就是在你的where语句中出现了between、&lt;、&gt;、in等的查询这种范围扫描索引扫描比全表扫描要好，因为它只需要开始于索引的某一点，而结束语另一点，不用扫描全部索引。<font color='cornflowerblue'>只扫描了一部分索引</font></p><p><img src="/Sql%E9%AB%98%E7%BA%A7/1646651193641.png" alt="1646651193641"><img src="/Sql%E9%AB%98%E7%BA%A7/1646651197122.png" alt="1646651197122"></p><p>**<font color='green'>index</font>:**出现index是sql使用了索引但是<font color='red'>没用通过索引进行过滤</font>，一般是使用了<font color='red'>覆盖索引</font>或者是<font color='red'>利用索引进行了排序分组</font>。<font color='cornflowerblue'>扫描了所有索引</font></p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646651286936.png" class="" width="1646651286936"><p>**<font color='cornflowerblue'>all</font>:**Full Table Scan，将遍历全表以找到匹配的行</p><img src="/2022/03/11/Sql%E9%AB%98%E7%BA%A7/1646651319155.png" class="" width="1646651319155"><p><strong><font color='red'>备注：一般来说，得保证查询至少达到range级别，最好能达到ref。</font></strong></p></blockquote></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;索引&quot;&gt;&lt;a href=&quot;#索引&quot; class=&quot;headerlink&quot; title=&quot;索引&quot;&gt;&lt;/a&gt;索引&lt;/h1&gt;&lt;p&gt;索引：&lt;font color=&#39;red&#39;&gt;排好序的快速查找数据结构&lt;/font&gt;&lt;/p&gt;
&lt;h2 id=&quot;BTree索引：&quot;&gt;&lt;a href</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>SQL基础</title>
    <link href="http://example.com/2022/03/09/SQL%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2022/03/09/SQL%E5%9F%BA%E7%A1%80/</id>
    <published>2022-03-09T11:49:06.000Z</published>
    <updated>2022-03-12T02:24:44.472Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MYSQL函数"><a href="#MYSQL函数" class="headerlink" title="MYSQL函数"></a>MYSQL函数</h1><p><code>select 函数名（） from 表 </code></p><h2 id="单行函数："><a href="#单行函数：" class="headerlink" title="单行函数："></a><strong>单行函数：</strong></h2><h3 id="字符函数："><a href="#字符函数：" class="headerlink" title="字符函数："></a>字符函数：</h3><ul><li>length（str）：返回参数值的字节个数</li><li>concat（str1，‘-‘，str2）： 拼接字符串</li><li>upper：转大写</li><li>lower：转小写</li><li>substr（str，num1,num2）（str，num）：截取字符串</li><li>instr（str1，str2）：返回str2在str1里的起始索引，找不到返回0</li><li>trim（str）（c from str）：去前后空格（c）</li><li>lpad（str，num，c）：用指定的字符实现<font color='red'>左</font>填充指定长度</li><li>rpad（str，num，c）：用指定的字符实现<font color='red'>右</font>填充指定长度</li><li>replace（str1，str2，str3）：用str3替换str1里面存在的str2字符串</li></ul><h3 id="数学函数："><a href="#数学函数：" class="headerlink" title="数学函数："></a>数学函数：</h3><ul><li>round（num，x）：四舍五入保留x位小数</li><li>ceil（num）：向上取整，返回&gt;&#x3D;num的最小整数 ，1.0001也是2</li><li>floor（num）:向下取整</li><li>truncate（num，x）：截断，小数后保留x位</li><li>mod（num，x）：取余   相当于  %    num%x</li></ul><h3 id="日期函数："><a href="#日期函数：" class="headerlink" title="日期函数："></a>日期函数：</h3><ul><li><p>now（）：返回当前时间 包括时间</p></li><li><p>curdate（）：返回日期不包含时间</p></li><li><p>curtime（）：返回时间，不包括日期</p></li><li><p>year（now（））：返回年。 月（month、monthname（月名字））） 日   小时  分钟  秒一样</p></li><li><p>str_to_date:将日期格式的字符转换为指定格式 。 <font color='green'>str_to_date(‘9-13-1993’,’%m-%d-%Y’); —-  1993-09-13</font></p><img src="/2022/03/09/SQL%E5%9F%BA%E7%A1%80/1646827945093.png" class="" width="1646827945093"></li><li><p>date_format:将日期转换为字符串     <font color='green'>date_format(now(),’%y年%每月%d日’)；   22年09月09日</font>其他函数：</p></li><li><p>version():查看版本号</p></li><li><p>database（）：查看当前数据库</p></li><li><p>user（）：查看当前用户</p></li></ul><h3 id="流程控制函数："><a href="#流程控制函数：" class="headerlink" title="流程控制函数："></a>流程控制函数：</h3><ul><li><p>if函数：if   else    的效果</p><p><font color='green'>select if （10 &gt; 5  ,’da’,’xiao’);     da</font>\</p></li><li><p>case函数     <font color='green'>select  原始工资，dep_id，case dep_id when 30 then salary * 1.1 when 40 then salary*1.2 else salary end as 新工资 from emp；</font></p></li></ul><h2 id="分组函数："><a href="#分组函数：" class="headerlink" title="分组函数："></a>分组函数：</h2><p>用做统计使用，又称聚合函数或组函数</p><p>有  <font color='green'> sum  avg  count max  min </font>。</p><p>特点：</p><ul><li>sum，avg一般用于处理数值函数</li><li>max ，min，count 可 用于任何函数</li><li>所有分组函数都忽略null值</li><li>可以和distinct（去重）搭配：<ul><li><font color='green'>select sum(distinct salary) ,sum(salary) from emp;    39666    1520</font></li></ul></li><li>count函数的详细介绍<ul><li>count(*) 查询总行数  count（常量） 也是统计总行数，其就是添加了一列都是常量</li><li>MYISAM存储下，count（*）的效率高  INNODB存储下  count(**)与 count(1)一样</li></ul></li><li>和分组函数一同查询的字段有限制，要求是group  by  后的字段</li></ul><h2 id="分页查询（limit）："><a href="#分页查询（limit）：" class="headerlink" title="分页查询（limit）："></a>分页查询（limit）：</h2><p>应用场景：当要显示的数据，一页显示不全，需要分页提交sql请求。</p><p>limit（offset，size）：offset要显示的起始索引<font color='red'>（从0开始）</font>，size要显示的条目个数</p><p><strong>特点：</strong></p><ul><li>limit语句要放在查询的最后</li><li>公式：<ul><li>要显示的页数page，每页的条目数size。 select 查询列表  from  表  limit （page-1）*size，size；</li></ul></li></ul><p>查询语句中的关键字和顺序：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select *         7</span><br><span class="line">from 表          1</span><br><span class="line">join 表2         2</span><br><span class="line">on 连接条件       3</span><br><span class="line">where  筛选条件   4</span><br><span class="line">group by 字段    5</span><br><span class="line">having  筛选条    6</span><br><span class="line">order by  字     8</span><br><span class="line">limit            9</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MYSQL函数&quot;&gt;&lt;a href=&quot;#MYSQL函数&quot; class=&quot;headerlink&quot; title=&quot;MYSQL函数&quot;&gt;&lt;/a&gt;MYSQL函数&lt;/h1&gt;&lt;p&gt;&lt;code&gt;select 函数名（） from 表 &lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;单行函数</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Yarn</title>
    <link href="http://example.com/2022/03/06/yarn/"/>
    <id>http://example.com/2022/03/06/yarn/</id>
    <published>2022-03-06T12:39:43.213Z</published>
    <updated>2022-03-06T15:16:12.281Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Yarn-资源调度器"><a href="#Yarn-资源调度器" class="headerlink" title="Yarn 资源调度器"></a><strong>Yarn</strong> <strong>资源调度器</strong></h2><p>​    Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式 </p><p>的操作系统平台，而 <font color='red'>MapReduce</font> 等运算程序则相当于运行于操作系统之上的应用程序。</p><h3 id="YARN基础架构"><a href="#YARN基础架构" class="headerlink" title="YARN基础架构"></a>YARN基础架构</h3><img src="/2022/03/06/yarn/1646531308187.png" class="" width="1646531308187"><h2 id="Yarn-工作机制"><a href="#Yarn-工作机制" class="headerlink" title="Yarn 工作机制"></a>Yarn 工作机制</h2><img src="/2022/03/06/yarn/1646531545692.png" class="" width="1646531545692"><p>（1）MR 程序提交到客户端所在的节点。 </p><p>（2）YarnRunner 向 ResourceManager 申请一个 Application。 </p><p>（3）RM 将该应用程序的资源路径返回给 YarnRunner。 </p><p>（4）该程序将运行所需资源提交到 HDFS 上。 </p><p>（5）程序资源提交完毕后，申请运行 mrAppMaster。 </p><p>（6）RM 将用户的请求初始化成一个 Task。 </p><p>（7）其中一个 NodeManager 领取到 Task 任务。 </p><p>（8）该 NodeManager 创建容器 Container，并产生 MRAppmaster。</p><p>（9）Container 从 HDFS 上拷贝资源到本地。 </p><p>（10）MRAppmaster 向 RM 申请运行 MapTask 资源。 </p><p>（11）RM 将运行 MapTask 任务分配给另外两个 NodeManager，另两个 NodeManager 分 </p><p>别领取任务并创建容器。 </p><p>（12）MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager </p><p>分别启动 MapTask，MapTask 对数据分区排序。 </p><p>（13）MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器，运行 ReduceTask。 </p><p>（14）ReduceTask 向 MapTask 获取相应分区的数据。 </p><p>（15）程序运行完毕后，MR 会向 RM 申请注销自己。</p><h3 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a><strong>作业提交全过程</strong></h3><img src="/2022/03/06/yarn/1646532204418.png" class="" width="1646532204418"><img src="/2022/03/06/yarn/1646563347178.png" class="" width="1646563347178"><img src="/2022/03/06/yarn/1646532229497.png" class="" width="1646532229497"><p>作业提交全过程详解 </p><p>（1）作业提交 </p><p>第 1 步：Client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业。 </p><p>第 2 步：Client 向 RM 申请一个作业 id。 </p><p>第 3 步：RM 给 Client 返回该 job 资源的提交路径和作业 id。 </p><p>第 4 步：Client 提交 jar 包、切片信息和配置文件到指定的资源提交路径。 </p><p>第 5 步：Client 提交完资源后，向 RM 申请运行 MrAppMaster。 </p><p>（2）作业初始化 </p><p>第 6 步：当 RM 收到 Client 的请求后，将该 job 添加到容量调度器中。</p><p>第 7 步：某一个空闲的 NM 领取到该 Job。 </p><p>第 8 步：该 NM 创建 Container，并产生 MRAppmaster。 </p><p>第 9 步：下载 Client 提交的资源到本地。 </p><p>（3）任务分配 </p><p>第 10 步：MrAppMaster 向 RM 申请运行多个 MapTask 任务资源。 </p><p>第 11 步：RM 将运行 MapTask 任务分配给另外两个 NodeManager，另两个 NodeManager </p><p>分别领取任务并创建容器。 </p><p>（4）任务运行 </p><p>第 12 步：MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 </p><p>NodeManager 分别启动 MapTask，MapTask 对数据分区排序。 </p><p>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。 </p><p>第 14 步：ReduceTask 向 MapTask 获取相应分区的数据。 </p><p>第 15 步：程序运行完毕后，MR 会向 RM 申请注销自己。 </p><p>（5）进度和状态更新 </p><p>​    YARN 中的任务将其进度和状态(包括 counter)返回给应用管理器, 客户端每秒(通过 </p><p>mapreduce.client.progressmonitor.pollinterval 设置)向应用管理器请求进度更新, 展示给用户。 </p><p>（6）作业完成 </p><p>​    除了向应用管理器请求作业进度外, 客户端每 5 秒都会通过调用 waitForCompletion()来 </p><p>检查作业是否完成。时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业 </p><p>完成之后, 应用管理器和 Container 会清理工作状态。作业的信息会被作业历史服务器存储 </p><p>以备之后用户核查。</p><h2 id="Yarn-调度器和调度算法"><a href="#Yarn-调度器和调度算法" class="headerlink" title="Yarn 调度器和调度算法"></a><strong>Yarn</strong> <strong>调度器和调度算法</strong></h2><p>Hadoop3.1.3默认的使用的调度器是Capacity Scheduler（容量）。</p><h4 id="先进先出调度器（FIFO）"><a href="#先进先出调度器（FIFO）" class="headerlink" title="先进先出调度器（FIFO）"></a>先进先出调度器（FIFO）</h4><img src="/2022/03/06/yarn/1646532413816.png" class="" width="1646532413816"><p><strong>优点：简单易懂；</strong> </p><p><strong>缺点：不支持多队列，生产环境很少使用；</strong></p><h4 id="容量调度器（Capacity-Scheduler）"><a href="#容量调度器（Capacity-Scheduler）" class="headerlink" title="容量调度器（Capacity Scheduler）"></a>容量调度器（Capacity Scheduler）</h4><p><img src="/yarn/1646532503851.png" alt="1646532503851"><img src="/yarn/1646532509897.png" alt="1646532509897"></p><h4 id="公平调度器（Fair-Scheduler）"><a href="#公平调度器（Fair-Scheduler）" class="headerlink" title="公平调度器（Fair Scheduler）"></a>公平调度器（Fair Scheduler）</h4><img src="/2022/03/06/yarn/1646533027061.png" class="" width="1646533027061"><img src="/2022/03/06/yarn/1646563372019.png" class="" width="1646533129528"><h4 id="公平调度器资源分配算法"><a href="#公平调度器资源分配算法" class="headerlink" title="公平调度器资源分配算法"></a><strong>公平调度器资源分配算法</strong></h4><h4 id="公平调度器队列资源分配方式"><a href="#公平调度器队列资源分配方式" class="headerlink" title="公平调度器队列资源分配方式"></a><img src="/yarn/1646533266234.png" alt="1646533266234"><strong>公平调度器队列资源分配方式</strong><img src="/yarn/1646533387333.png" alt="1646533387333"></h4><p>3）DRF策略 </p><p>3）DRF策略<br>    DRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是Yarn默<br>认的情况）。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用<br>应该分配的资源比例。<br>    那么在YARN中，我们用DRF来决定如何调度：<br>    假设集群一共有100 CPU和10T 内存，而应用A需要（2 CPU, 300GB），应用B需要（6 CPU，100GB）。<br>则两个应用分别需要A（2%CPU, 3%内存）和B（6%CPU, 1%内存）的资源，这就意味着A是内存主导的, B是<br>CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比<br>例的限制</p><h3 id="Yarn-生产环境核心参数"><a href="#Yarn-生产环境核心参数" class="headerlink" title="Yarn 生产环境核心参数"></a>Yarn 生产环境核心参数</h3><img src="/2022/03/06/yarn/1646534533223.png" class="" width="1646534533223"><h3 id="容量调度器多队列提交案例"><a href="#容量调度器多队列提交案例" class="headerlink" title="容量调度器多队列提交案例"></a><strong>容量调度器多队列提交案例</strong></h3><p>1）在生产环境怎么创建队列？</p><p>（1）调度器默认就1个default队列，不能满足生产要求。</p><p>（2）按照框架：hive &#x2F;spark&#x2F; flink 每个框架的任务放入指定的队列（企业用的不是特别多）</p><p>（3）按照业务模块：登录注册、购物车、下单、业务部门1、业务部门2</p><p>2）创建多队列的好处？</p><p>（1）因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。</p><p>（2）实现任务的<strong>降级</strong>使用，特殊时期保证重要的任务队列资源充足。11.11 6.18</p><p>业务部门1（重要）&#x3D;》业务部门2（比较重要）&#x3D;》下单（一般）&#x3D;》购物车（一般）&#x3D;》登录注册（次要）</p><p><strong>重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</strong></p><p><code>yarn rmadmin -refreshQueues</code></p><p><strong>提交队伍的方式</strong></p><p>1）hadoop jar的方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output</span><br></pre></td></tr></table></figure><p><font color='red'>-D mapreduce.job.queuename&#x3D;hive</font></p><p>2）打jar包的方式</p><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：.</p><img src="/2022/03/06/yarn/1646538048009.png" class="" width="1646538048009"><h3 id="任务优先级"><a href="#任务优先级" class="headerlink" title="任务优先级"></a>任务优先级</h3><p>​    容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn将所有任务的优先级限制为0，若想使用任务的优先级功能，须开放该限制。</p><p>1）修改yarn-site.xml文件，增加以下参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）分发配置，并重启Yarn</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> xsync yarn-site.xml</span><br><span class="line">[ hadoop-3.1.3]$ sbin/stop-yarn.sh</span><br><span class="line">[ hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>3）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop-3.1.3]$hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000</span><br></pre></td></tr></table></figure><img src="/2022/03/06/yarn/1646538180009.png" class="" width="1646538180009"><p>4）再次重新提交优先级高的任务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop-3.1.3]$hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000</span><br></pre></td></tr></table></figure><img src="/2022/03/06/yarn/1646538197679.png" class="" width="1646538197679"><p>5）也可以通过以下命令修改正在执行的任务的优先级。</p><p><code>yarn application -appID &lt;ApplicationID&gt; -updatePriority 优先级</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5</span><br></pre></td></tr></table></figure><h3 id="公平调度器案例"><a href="#公平调度器案例" class="headerlink" title="公平调度器案例"></a>公平调度器案例</h3><p>​    创建两个队列，分别是 test 和 atguigu（以用户所属组命名）。期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，tes t用户提交的任务到 root.group.test 队列运行，atguigu 提交的任务到 root.group.atguigu 队列运行（注：group为用户所属组）。<br>​    公平调度器的配置涉及到两个文件，一个是 yarn-site.xml，另一个是公平调度器队列分配文件fair-scheduler.xml（文件名可自定义）。</p><p>1）修改yarn-site.xml文件，加入以下参数</p><blockquote><property>    <name>yarn.resourcemanager.scheduler.class</name>    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>    <description>配置使用公平调度器</description></property><property>   <name>yarn.scheduler.fair.allocation.file</name>    <value>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml</value>    <description>指明公平调度器队列分配配置文件</description></property><property>    <name>yarn.scheduler.fair.preemption</name>    <value>false</value>    <description>禁止队列间资源抢占</description></property></blockquote><p>2）配置fair-scheduler.xml</p><blockquote><?xml version="1.0"?><allocations>  <!-- 单个队列中Application Master占用资源的最大比例,取值0-1 ，企业一般配置0.1 -->  <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>  <!-- 单个队列最大资源的默认值 test atguigu default -->  <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>  <!-- 增加一个队列test -->  <queue name="test">    <!-- 队列最小资源 -->    <minResources>2048mb,2vcores</minResources>    <!-- 队列最大资源 -->    <maxResources>4096mb,4vcores</maxResources>    <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->    <maxRunningApps>4</maxRunningApps>    <!-- 队列中Application Master占用资源的最大比例 -->    <maxAMShare>0.5</maxAMShare>    <!-- 该队列资源权重,默认值为1.0 -->    <weight>1.0</weight>    <!-- 队列内部的资源分配策略 -->    <schedulingPolicy>fair</schedulingPolicy>  </queue>  <!-- 增加一个队列atguigu -->  <queue name="atguigu" type="parent">    <!-- 队列最小资源 -->    <minResources>2048mb,2vcores</minResources>    <!-- 队列最大资源 -->    <maxResources>4096mb,4vcores</maxResources>    <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->    <maxRunningApps>4</maxRunningApps>    <!-- 队列中Application Master占用资源的最大比例 -->    <maxAMShare>0.5</maxAMShare>    <!-- 该队列资源权重,默认值为1.0 -->    <weight>1.0</weight>    <!-- 队列内部的资源分配策略 -->    <schedulingPolicy>fair</schedulingPolicy>  </queue>  <!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 -->  <queuePlacementPolicy>    <!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建-->    <rule name="specified" create="false"/>    <!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 -->    <rule name="nestedUserQueue" create="true">        <rule name="primaryGroup" create="false"/>    </rule>    <!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 -->    <rule name="reject" />  </queuePlacementPolicy></allocations></blockquote><p>3）分发配置并重启Yarn</p><h2 id="Yarn的Tool接口案例"><a href="#Yarn的Tool接口案例" class="headerlink" title="Yarn的Tool接口案例"></a>Yarn的Tool接口案例</h2><p>0）回顾：</p><p>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.<font color='red'>wordcount2</font>.WordCountDriver &#x2F;input &#x2F;output1</p><p>期望可以动态传参，结果报错，误认为是第一个输入参数。</p><p>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.<font color='red'>wordcount2</font>.WordCountDriver <font color='red'>-Dmapreduce.job.queuename&#x3D;root.test</font> &#x2F;input &#x2F;output1</p><p>1）需求：自己写的程序也可以动态修改参数。编写Yarn的Tool接口。</p><p>2）具体步骤：</p><p>（1）创建类WordCount并实现Tool接口：</p><p>​        里面有三个方法分别是 run（）  setConf（） getConf（） 在run里面实现Driver里面的7步</p><p>​        Mapper类和Reducer类 创建为内部类，内容一样</p><p>（4）新建WordCountDriver</p><p>​    在里面获取 conf  判断是否有Tool接口</p><p>  &#x2F;&#x2F; 1. 创建配置文件<br>        Configuration conf &#x3D; new Configuration();</p><pre><code>    // 2. 判断是否有tool接口    switch (args[0])&#123;        case &quot;wordcount&quot;:            tool = new WordCount();            break;        default:            throw new RuntimeException(&quot; No such tool: &quot;+ args[0] );    &#125;    // 3. 用Tool执行程序    // Arrays.copyOfRange 将老数组的元素放到新数组里面    int run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, 1, args.length));    System.exit(run);</code></pre><p>在hadoop上运行时要加上特定的Tool接口</p><p>[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver  <font color='red'>wordcount</font>  &#x2F;input &#x2F;output </p><p>注意此时提交的3个参数，第一个用于生成特定的Tool，第二个和第三个为输入输出目录。此时如果我们希望加入设置参数，可以在wordcount后面添加参数，例如：</p><p>[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver  <font color='red'>wordcount -Dmapreduce.job.queuename&#x3D;root.test</font>  &#x2F;input &#x2F;output1</p><h2 id="Yarn重点"><a href="#Yarn重点" class="headerlink" title="Yarn重点"></a>Yarn重点</h2><h3 id="1-Yarn的工作机制"><a href="#1-Yarn的工作机制" class="headerlink" title="1.Yarn的工作机制"></a>1.Yarn的工作机制</h3><h3 id="2-Yarn的调度器"><a href="#2-Yarn的调度器" class="headerlink" title="2.Yarn的调度器"></a>2.Yarn的调度器</h3><p>​    1）FIFO&#x2F;容量&#x2F;公平</p><p>​    2）apache 默认调度器是容量   容量： CDH默认调度器  公平</p><p>​    3）公平&#x2F;容量 默认一个default，需要创建多队列</p><p>​    4）中小企业：hive   spark  flink  mr</p><p>​    5）中大企业：业务模块 ：登录&#x2F;注册&#x2F;购物车&#x2F;营销</p><p>​    6）好处：解耦  降低风险 降级使用</p><p>​    7）每个调度器特点</p><p>​            相同点：支持多队列，可以借资源，支持多用户</p><p>​            不同点：容量：优先满足先进来的任务</p><p>​                            公平：公平，在队列里面公平享有队列资源</p><p>​    8）怎么选：</p><p>​                中小企业：对并发度要求不高，选容量</p><p>​                中大企业：对并发度比较高，选公平</p><h3 id="3-开发需要重点掌握"><a href="#3-开发需要重点掌握" class="headerlink" title="3.开发需要重点掌握"></a>3.开发需要重点掌握</h3><p>1）调度算法</p><p>2）Yarn常用命令（查看日志，APP…的运行，查看容器）</p><p>3）核心参数的配置</p><p>4）配置容量调度器和公平调度器</p><p>5）Tool接口使用</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Yarn-资源调度器&quot;&gt;&lt;a href=&quot;#Yarn-资源调度器&quot; class=&quot;headerlink&quot; title=&quot;Yarn 资源调度器&quot;&gt;&lt;/a&gt;&lt;strong&gt;Yarn&lt;/strong&gt; &lt;strong&gt;资源调度器&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;​   </summary>
      
    
    
    
    <category term="Hadoop" scheme="http://example.com/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Java类</title>
    <link href="http://example.com/2022/03/06/java%E7%B1%BB/"/>
    <id>http://example.com/2022/03/06/java%E7%B1%BB/</id>
    <published>2022-03-06T10:45:31.461Z</published>
    <updated>2022-03-06T15:17:45.246Z</updated>
    
    <content type="html"><![CDATA[<h3 id="System类"><a href="#System类" class="headerlink" title="System类"></a>System类</h3><p><strong>范例</strong>:在开发中统计耗时</p><p><code>public static long currentTimeMillis();</code></p><h3 id="Clearn类"><a href="#Clearn类" class="headerlink" title="Clearn类"></a>Clearn类</h3><p>1.9版本之后提供的一个对象清理的类，其主要功能是finialize()方法的替代。</p><h3 id="大数字类处理"><a href="#大数字类处理" class="headerlink" title="大数字类处理"></a>大数字类处理</h3><p><strong>进行超过了double范围类型的数字处理</strong>。两个大数字操作类 BigInteger 与 BigDecimal类</p><img src="/2022/03/06/java%E7%B1%BB/1645946509291.png" class="" width="1645946509291"><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><table><thead><tr><th>public boolean matches(String regex)</th><th>普通</th><th>将指定字符串进行正则判断</th></tr></thead><tbody><tr><td>public String replaceAll(String regex,String replacement)</td><td>普通</td><td>根据正则表达式替换全部</td></tr><tr><td>public String replaceFirst(String regex,String replacement)</td><td>普通</td><td>根据正则表达式替首个</td></tr><tr><td>public String[] split(String regex)</td><td>普通</td><td>正则拆分</td></tr><tr><td>public String[] split(String regex，int limit)</td><td>普通</td><td>正则拆分</td></tr></tbody></table><h3 id="Comparable"><a href="#Comparable" class="headerlink" title="Comparable"></a>Comparable</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Comparable</span>&lt;T&gt;&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 实现对象的比较处理操作</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> o 要比较的对象</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> 当前数据比传入的对象小返回负数，如果大于返回整数，如果等于返回0</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(T o)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  排序里面只需要一个compareTo()方法进行排序规则的定义，而后整个Java系统里面就可以为其实现排序处理了。 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">JavaAPIDemmo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ParseException  &#123;</span><br><span class="line">      Person data[] = <span class="keyword">new</span> <span class="title class_">Person</span>[] &#123;</span><br><span class="line">          <span class="keyword">new</span> <span class="title class_">Person</span>(<span class="string">&quot;小强A&quot;</span>, <span class="number">80</span>),</span><br><span class="line">          <span class="keyword">new</span> <span class="title class_">Person</span>(<span class="string">&quot;小强B&quot;</span>, <span class="number">100</span>),</span><br><span class="line">          <span class="keyword">new</span> <span class="title class_">Person</span>(<span class="string">&quot;小强C&quot;</span>, <span class="number">50</span>)&#125;;</span><br><span class="line">          Arrays.sort(data);</span><br><span class="line">          System.out.println(Arrays.toString(data));</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span> <span class="keyword">implements</span> <span class="title class_">Comparable</span>&lt;Person&gt;&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Person</span><span class="params">(String name,<span class="type">int</span> age)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(Person per)</span> &#123;</span><br><span class="line">      <span class="comment">/*  if(this.age &lt; per.age)&#123;</span></span><br><span class="line"><span class="comment">            return -1;</span></span><br><span class="line"><span class="comment">        &#125;else if(this.age &gt; per.age)&#123;</span></span><br><span class="line"><span class="comment">            retrun 1;</span></span><br><span class="line"><span class="comment">        &#125;else&#123;</span></span><br><span class="line"><span class="comment">            return 0;</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.age - per.age; <span class="comment">//升序</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;【Person类对象】姓名：&quot;</span>+<span class="built_in">this</span>.name+<span class="string">&quot;、年龄：&quot;</span>+<span class="built_in">this</span>.age+<span class="string">&quot;\n&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">[【Person类对象】姓名：小强C、年龄：50</span></span><br><span class="line"><span class="comment">, 【Person类对象】姓名：小强A、年龄：80</span></span><br><span class="line"><span class="comment">, 【Person类对象】姓名：小强B、年龄：100</span></span><br><span class="line"><span class="comment">]</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h3 id="File类"><a href="#File类" class="headerlink" title="File类"></a>File类</h3><p><strong>在文件创建前要确保其父路径存在</strong>,父路径不存在可以用.mkdirs()来创建父目录</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!file.getParentFile().exists()) &#123;<span class="comment">//父路径不存在</span></span><br><span class="line">file.getParentFile( ).mkdirs() ; /创建父路径</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>保留两位小数</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MathUtil</span>()&#123;</span><br><span class="line">    pribate <span class="title function_">MathUtil</span><span class="params">()</span>&#123;&#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span> <span class="title function_">round</span><span class="params">(<span class="type">double</span> num ,<span class="type">int</span> scale)</span>&#123;<span class="comment">//num是要保留的数字，scale是保留几位小数</span></span><br><span class="line">        retrun Math.round(Math.pow(<span class="number">10</span>,scale) * num) / Math.pow(<span class="number">10</span>,scale);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字节流和字符流"><a href="#字节流和字符流" class="headerlink" title="字节流和字符流"></a>字节流和字符流</h3><p>对于服务器或者是客户端而言实质上传递的就是一种数据流的处理形式，而所谓的数据流指的就是字节数据。而对于流的处理形式在java.io包里面提供有两类支持：</p><ul><li>字节处理流：OutputStream(输出字节流)、InputStream(输入字节流)；</li><li>字符处理流：Writer(输出字符流)、Reader(输入字符流)。</li></ul><h3 id="1-字节输出流：OutputStream"><a href="#1-字节输出流：OutputStream" class="headerlink" title="1.字节输出流：OutputStream"></a>1.字节输出流：OutputStream</h3><p>字节的数据是以byte类型为主实现的操作，在进行字节内容输出的时候可以使用OutputStream实现，这个类的基本定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">OutputStream</span> <span class="keyword">extends</span> <span class="title class_">Object</span> <span class="keyword">implements</span> <span class="title class_">Closeable</span>,FlushableCopy to clipboardErrorCopied</span><br></pre></td></tr></table></figure><p><strong>OutputStream类定义的是一个公共的输出操作标准，而在这个标准里面一共定义有：</strong></p><table><thead><tr><th>No.</th><th>方法名称</th><th>类型</th><th>特性</th></tr></thead><tbody><tr><td>01</td><td>public abstract void write(int b)throws IOException</td><td>普通</td><td>输出单个字节数据</td></tr><tr><td>02</td><td>public void write(byte[] b)throws IOException</td><td>普通</td><td>输出一组字节数据</td></tr><tr><td><strong>03</strong></td><td><strong>public void write(byte[] b,int off,int len)throws IOException</strong></td><td><strong>普通</strong></td><td><strong>输出部分字节数据</strong></td></tr></tbody></table><h3 id="2-字节输入流：InputStream"><a href="#2-字节输入流：InputStream" class="headerlink" title="2.字节输入流：InputStream"></a>2.字节输入流：InputStream</h3><p><a href="https://docs.oracle.com/en/java/javase/14/docs/api/java.base/java/io/InputStream.html">InputStream类</a>主要实现的就是字节数据读取，该类定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">InputStream</span> <span class="keyword">extends</span> <span class="title class_">Object</span> <span class="keyword">implements</span> <span class="title class_">CloseableCopy</span> to clipboardErrorCopied</span><br></pre></td></tr></table></figure><p><strong>在InputStream类里面定义有如下几个核心方法：</strong></p><table><thead><tr><th>No.</th><th>方法名称</th><th>类型</th><th>特性</th></tr></thead><tbody><tr><td>01</td><td>public abstract int read() throws IOException</td><td>普通</td><td>读取单个字节数据，如果现在已经读取到底了，返回-1</td></tr><tr><td><strong>02</strong></td><td><strong>public int read(byte[] b) throws IOException</strong></td><td><strong>普通</strong></td><td><strong>读取一组字节数据</strong></td></tr><tr><td>03</td><td>public int read(byte[] b, int off, int len) throws IOException</td><td>普通</td><td>读取一组字节数据(只占数组的部分)·</td></tr></tbody></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">JavaAPIDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;E:&quot;</span>+File.separator+<span class="string">&quot;hello&quot;</span>+File.separator+<span class="string">&quot;mldn.txt&quot;</span>);</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">input</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(file);</span><br><span class="line">        <span class="type">byte</span> data [] = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>]; <span class="comment">//开辟一个缓冲区读取数据</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> input.read(data); <span class="comment">//读取数据，数据全部保存在字节数组之中，返回读取个数</span></span><br><span class="line">        System.out.println(<span class="string">&quot;【&quot;</span>+<span class="keyword">new</span> <span class="title class_">String</span>(data,<span class="number">0</span>,len)+<span class="string">&quot;】&quot;</span>);</span><br><span class="line">        input.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;Copy to clipboardErrorCopied</span><br></pre></td></tr></table></figure><h3 id="3-字符输出流：Writer"><a href="#3-字符输出流：Writer" class="headerlink" title="3.字符输出流：Writer"></a>3.字符输出流：Writer</h3><ul><li>输出字符数组：public void write(char[] cbuf) throws IOException；</li><li>输出字符串：public void write(String str) throws [IOException]；</li></ul><h3 id="4-字符输入流：Reader"><a href="#4-字符输入流：Reader" class="headerlink" title="4.字符输入流：Reader"></a>4.字符输入流：Reader</h3><p>字符流读取的时候只能够按照数组的形式来实现处理操作。</p><ul><li>接受数据：public int read(char[] cbuf) throws IOException;</li></ul><h3 id="5-字符流和字节流的区别"><a href="#5-字符流和字节流的区别" class="headerlink" title="5.字符流和字节流的区别"></a>5.字符流和字节流的区别</h3><p>在使用OutputStream类输出的时候如果现在没有使用close()方法关闭输出流发现内容依然可以正常的输出。但如果在使用Writer输出的时候如果现在没有使用close()方法关闭输出流，那么这个时候内容将无法进行输出，因为Writer使用到了缓冲区。当使用close()方法的时候将会出现强制刷新缓冲区的情况，所以这个时候会将内容进行输出，如果没有关闭，那么将无法进行输出操作，所以此时如果在不关闭的情况下要想将全部的内容输出可以使用flush()方法强制清空。</p><p><strong>字节流在进行处理的时候并不会使用到缓冲区，而字符流会使用到缓冲区。另外那个缓冲区的字符流更加适合于中文数据的处理。</strong></p><p>结论：只要是纯文本数据优先使用字符流，除此之外都使用字节流。</p><h3 id="Java序列化"><a href="#Java序列化" class="headerlink" title="Java序列化"></a>Java序列化</h3><p><strong>用io包中的 Serializable接口就可实现序列化</strong></p><h3 id="HashMap"><a href="#HashMap" class="headerlink" title="HashMap"></a>HashMap</h3><table><thead><tr><th>当使用无参方法构造时会有一个loadFactor属性，并且默认为0.75</th></tr></thead><tbody><tr><td>使用put（）方法保存时会调用putVal()方法，而在putVal方法里面也有一个数据的保存，调用resize（）方法进行容量的扩充</td></tr></tbody></table><p><strong>是如何用put()扩充的？</strong></p><p>a.在 HashMap_类里面提供有一个“DEFAULT_ INITIAL_CAPACITY”常量，作为16个元素，也就是说默认可以保存的最大内容是16;</p><p>b.当保存的内容的容量超过了与个阈值“DEFAULT LOAD FACTOR &#x3D; 0.75f”，相当与“容量 * 阈值 &#x3D; 12“ 保存12个元素的时候就会进行容量的扩充;</p><p>c.在采用扩充的时候每次扩充2倍的容量   源代码&lt;&lt;1 向左移一</p><p><strong>请解释工作原理</strong></p><p>a.存储依然是用了Node类完成的    （链表（时间复杂度O(n）、二叉树（时间复杂度O(logn））。</p><p>b.从1.8起要适应大数据时代，所以存储发生了变化，提供了一个常量<code>TREEIFY THRESHOLD = 8</code>，如果阈值没有超过8那就会按照链表的方式进行处理，如果超过8则会链表转换为红黑树</p><p>通过HashMap实例化的Map接口可以针对与key或value<strong>保存null的数据</strong>，同时也可以发现即便保存数据的<strong>key重复</strong>，那么也不会出现错误，而是出现<strong>内容的替换</strong>。</p><p><strong>面试题：在进行HashMap的put()操作的时候，如何实现容量的扩充？</strong></p><ul><li>在HashMap类里面提供有一个“DEFAULT_INITIAL_CAPACITY”常量，作为初始化的容量配置，作为初始化大小为16个元素。</li><li>当我们保存的容量超过了一个阈值(DEFAULT_LOAD_FACTOP&#x3D;0.75f)，相当于容量*阈值&#x3D;12，保存12个元素的时候容量会扩充。</li><li>在进行扩充的时候HashMap采用的是成倍扩充模式。</li></ul><p><strong>HashMap是使用了哪些方法来有效解决哈希冲突？</strong></p><ol><li>使用 链地址法（使用散列表）来链接拥有相同hash值的数据；</li><li>使用 2次扰动函数（hash函数）来降低哈希冲突的概率，使得数据分布更平均；</li><li>引入红黑树进一步降低遍历的时间复杂度，使得遍历更快；</li></ol><p><strong>HashMap在JDK1.7和JDK1.8中有哪些不同</strong></p><table><thead><tr><th>不同</th><th>JDK 1.7</th><th>JDK 1.8</th></tr></thead><tbody><tr><td>存储结构</td><td>数组 + 链表</td><td>数组 + 链表 + 红黑树</td></tr><tr><td>初始化方式</td><td>单独函数：<code>inflateTable()</code></td><td>直接集成到了扩容函数<code>resize()</code>中</td></tr><tr><td>hash值计算方式</td><td>扰动处理 &#x3D; 9次扰动 &#x3D; 4次位运算 + 5次异或运算</td><td>扰动处理 &#x3D; 2次扰动 &#x3D; 1次位运算 + 1次异或运算</td></tr><tr><td>存放数据的规则</td><td>无冲突时，存放数组；冲突时，存放链表</td><td>无冲突时，存放数组；冲突 &amp; 链表长度 &lt;&#x3D; 8：存放单链表；冲突 &amp; 链表长度 &gt; 8：树化并存放红黑树</td></tr><tr><td>插入数据方式</td><td>头插法（先讲原位置的数据移到后1位，再插入数据到该位置）</td><td>尾插法（直接插入到链表尾部&#x2F;红黑树）</td></tr><tr><td>扩容后存储位置的计算方式</td><td>全部按照原来方法进行计算（即hashCode -&gt;&gt; 扰动函数 -&gt;&gt; (h&amp;length-1)）</td><td>按照扩容后的规律计算（即扩容后的位置&#x3D;原位置 or 原位置 + 旧容量）</td></tr></tbody></table><p><strong>为什么JDK1.8要用尾插法？</strong></p><p><strong>使用头插</strong>会改变链表的上的顺序，但是如果<strong>使用尾插</strong>，在扩容时会保持链表元素原本的顺序，就不会出现<strong>链表成环</strong>的问题了。</p><p>面试题：请解释HashMap与Hashtable的区别：</p><ul><li>HashMap中的方法都属于异步操作(非线程安全),HashMap允许保存有null数据；</li><li>Hashtable中的方法都属于同步方法(线程安全)，Hashtable不允许保存null，否则会出现NullPointException。</li></ul><p><strong>使用Iterator实现Map集合的输出步骤：</strong></p><ul><li><p>利用Map接口中提供的entrySet()方法将Map集合转为Set集合；</p></li><li><p>利用Set接口中的iterator()方法将Set集合转为Iterator接口实例；</p></li><li><p>利用Iterator进行迭代输出获取每一组Map.Entry对象，随后通过getKey()与getValue()获取数据</p><img src="/2022/03/06/java%E7%B1%BB/1646315474798.png" class="" width="1646315474798"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;Double,String&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>();</span><br><span class="line">     map.put(<span class="number">1d</span>,<span class="string">&quot;aaa&quot;</span>);<span class="comment">//添加</span></span><br><span class="line">     map.put(<span class="number">2d</span>,<span class="string">&quot;bbb&quot;</span>);</span><br><span class="line">     map.put(<span class="number">3d</span>,<span class="string">&quot;ccc&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (Object f:map.entrySet())&#123;</span><br><span class="line">         System.out.println(f);</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;System类&quot;&gt;&lt;a href=&quot;#System类&quot; class=&quot;headerlink&quot; title=&quot;System类&quot;&gt;&lt;/a&gt;System类&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;范例&lt;/strong&gt;:在开发中统计耗时&lt;/p&gt;
&lt;p&gt;&lt;code&gt;public s</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java类" scheme="http://example.com/tags/Java%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>链表，二叉树</title>
    <link href="http://example.com/2022/03/06/hello/"/>
    <id>http://example.com/2022/03/06/hello/</id>
    <published>2022-03-06T10:45:30.097Z</published>
    <updated>2022-03-06T15:17:41.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h1><h2 id="创建链表"><a href="#创建链表" class="headerlink" title="创建链表"></a>创建链表</h2><p><strong>用Node节点来进行数据保存，创建一个专门的类来进行管理，客户端不操作Node。</strong>应该对Node类进行包装处理，定义出操作标准。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> <span class="title class_">ILink</span>&lt;E&gt;&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(E e)</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">()</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isEmpty</span><span class="params">()</span>;</span><br><span class="line">    <span class="keyword">public</span> Object [] toArray();</span><br><span class="line">    <span class="keyword">public</span> E <span class="title function_">get</span><span class="params">(<span class="type">int</span> index)</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set</span><span class="params">(<span class="type">int</span> index,Object data)</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">contains</span><span class="params">(E data)</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">remove</span><span class="params">(E data)</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">clean</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinkImpl</span>&lt;E&gt; <span class="keyword">implements</span> <span class="title class_">ILink</span>&lt;E&gt;&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">Node</span>&#123;   <span class="comment">//保存节点的数据关系</span></span><br><span class="line">        <span class="keyword">private</span> E data;   <span class="comment">//保存数据</span></span><br><span class="line">        <span class="keyword">private</span> Node next;    <span class="comment">//保存下一个引用</span></span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">Node</span><span class="params">(E data)</span>&#123;  <span class="comment">//有数据才有意义</span></span><br><span class="line">            <span class="built_in">this</span>.data = data;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">addNode</span><span class="params">(Node newNode)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.next == <span class="literal">null</span>)&#123;</span><br><span class="line">                <span class="built_in">this</span>.next == newNode;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">this</span>.next.addNode(newNode)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//第一次调用： this = LinkImpl.root</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">toArrayNode</span><span class="params">()</span>&#123;</span><br><span class="line">            LinkImpl.<span class="built_in">this</span>.returnData[LinkImpl.<span class="built_in">this</span>.foot++] = <span class="built_in">this</span>.data;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.next)&#123;</span><br><span class="line">                <span class="built_in">this</span>.next.toArrayNode();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> E <span class="title function_">getNode</span><span class="params">(<span class="type">int</span> index)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(LinkImpl.<span class="built_in">this</span>.foot++ == index)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">this</span>.data;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">this</span>.next.getNode(index);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setNode</span><span class="params">(<span class="type">int</span> index,Object data)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(LinkImpl.<span class="built_in">this</span>.foot++ == index)&#123;</span><br><span class="line">                <span class="built_in">this</span>.data = data;  <span class="comment">//修改数据</span></span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">this</span>.next.setNode(index,data);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">containsNode</span><span class="params">(Object data)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.data.equals(data))&#123;    <span class="comment">//对象比较</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">this</span>.next==<span class="literal">null</span>)&#123;   <span class="comment">//没有后续节点</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                    retrun  <span class="built_in">this</span>.next.containsNode(data);  <span class="comment">//向后继续寻找</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">removeNode</span><span class="params">(Node pre , E data)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.data.equals(data))&#123;</span><br><span class="line">                pre.next = <span class="built_in">this</span>.next;  <span class="comment">//空出当前节点</span></span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">this</span>.next!=<span class="literal">null</span>)&#123;  <span class="comment">//有后续节点</span></span><br><span class="line">                    <span class="built_in">this</span>.next.removeNode(<span class="built_in">this</span>,data);  <span class="comment">//向后继续删除</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//  -----以下为Link类中定义的成员-----</span></span><br><span class="line">    <span class="keyword">private</span> Node root;  <span class="comment">//保存根元素</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> count;  <span class="comment">//统计个数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> foot;  <span class="comment">//定义下标</span></span><br><span class="line">    <span class="keyword">private</span> Object [] returnData;  <span class="comment">//定义返回数组</span></span><br><span class="line">    <span class="comment">//  -----以下为Link类中定义的方法-----</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(E e)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(e==<span class="literal">null</span>)&#123;   <span class="comment">//如果保存数据为空至极返回</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Node newNode <span class="keyword">new</span> <span class="title class_">Node</span>(e);  <span class="comment">//创建一个新的节点</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">this</span>.root == <span class="literal">null</span>)&#123;       </span><br><span class="line">            <span class="built_in">this</span>.root = newNode;   <span class="comment">//如果节点为空则将新的节点作为根节点</span></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">this</span>.root.addNode(newNode);    <span class="comment">//否则存入下一节点</span></span><br><span class="line">        &#125;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isEmpty</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count==<span class="number">0</span>;     <span class="comment">// 1.判断数据是否为0</span></span><br><span class="line">       <span class="comment">//return root==null;  //2.判断根节点是否为空</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Object [] toArray()&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">this</span>.root==<span class="literal">null</span>)&#123;    <span class="comment">//如果为空直接返回</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">this</span>.foot = <span class="number">0</span>;  <span class="comment">//脚标清零</span></span><br><span class="line">        <span class="built_in">this</span>.returnData = <span class="keyword">new</span> <span class="title class_">Object</span>[<span class="built_in">this</span>.count];  <span class="comment">//根据已有长度开辟数组</span></span><br><span class="line">        <span class="built_in">this</span>.root.toArrayNode();  <span class="comment">//进行递归保存数据</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.returnData;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> E <span class="title function_">get</span><span class="params">(<span class="type">int</span> index)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(index &gt;= <span class="built_in">this</span>.count)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">this</span>.foot = <span class="number">0</span>; <span class="comment">//索引清零</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.root.getNode(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set</span><span class="params">(<span class="type">int</span> index,Object data)</span>&#123;</span><br><span class="line">         <span class="keyword">if</span>(index &gt;= <span class="built_in">this</span>.count)&#123;</span><br><span class="line">            <span class="keyword">return</span>;  <span class="comment">//退出</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">this</span>.foot = <span class="number">0</span>; <span class="comment">//索引清零</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.root.setNode(index,data);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">contains</span><span class="params">(Object data)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">null</span>) </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;  <span class="comment">//没有数据</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.root.containsNode(data)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">remove</span><span class="params">(E data)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">this</span>.contains(data))&#123;  <span class="comment">//判断数据是否存在</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.root.data.equals(data))&#123;  <span class="comment">//判断是否为根节点</span></span><br><span class="line">                <span class="built_in">this</span>.root = <span class="built_in">this</span>.root.next;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">this</span>.root.next.removeNode(<span class="built_in">this</span>.root,data);  <span class="comment">//交个Node类来处理</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">this</span>.count--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">clean</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="built_in">this</span>.root = <span class="literal">null</span>;</span><br><span class="line">        <span class="built_in">this</span>.count = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h1><h2 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ListNode <span class="title function_">reverse</span><span class="params">(ListNode head)</span> &#123;   <span class="comment">//head  : 1-&gt;2-&gt;3</span></span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">prev</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">while</span> (head != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">next</span> <span class="operator">=</span> head.next;  <span class="comment">//指向下一个节点  2-&gt;3  // 3</span></span><br><span class="line">        head.next = prev;  <span class="comment">//head.next = null;    //head.next = 1</span></span><br><span class="line">        prev = head;    <span class="comment">// prev = 1 -&gt;null        //prev = 2-&gt;1-&gt;null</span></span><br><span class="line">        head = next;   <span class="comment">//  head = 2-&gt;3            //head = 3</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> prev;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>主要思想为创建一个新的链表，让head节点的内容一直指向新链表的头部</li></ul><h1 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h1><h2 id="二叉树的深度"><a href="#二叉树的深度" class="headerlink" title="二叉树的深度"></a>二叉树的深度</h2><h3 id="1-遍历"><a href="#1-遍历" class="headerlink" title="1.遍历"></a>1.遍历</h3><p>+++</p><h4 id="a-先序遍历："><a href="#a-先序遍历：" class="headerlink" title="a.先序遍历："></a>a.先序遍历：</h4><p> 他的访问顺序是：根节点→左子树→右子树 ；</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">preOrder</span><span class="params">(TreeNode tree)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(tree==<span class="literal">null</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    System.out.println(tree.val+<span class="string">&quot; &quot;</span>);</span><br><span class="line">    preOrder(root.left);</span><br><span class="line">    preOrder(root.right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>:carousel_horse:</p><h4 id="b-中序遍历"><a href="#b-中序遍历" class="headerlink" title="b.中序遍历"></a>b.中序遍历</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">inOrder</span><span class="params">(TreeNode tree)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(tree==<span class="literal">null</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    inOrder(root.left);</span><br><span class="line">    System.out.println(root.val+<span class="string">&quot; &quot;</span>);</span><br><span class="line">    inOrder(root.right);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>:dog:</p><h4 id="c-后序遍历"><a href="#c-后序遍历" class="headerlink" title="c.后序遍历"></a>c.后序遍历</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">lasrOrder</span><span class="params">(TreeNode tree)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(tree==<span class="literal">null</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    lastOrder(tree.left);</span><br><span class="line">    lastOrder(tree.right);</span><br><span class="line">    System.out.println(root.val+<span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>:cat2:</p><h3 id="2-递归求深度"><a href="#2-递归求深度" class="headerlink" title="2.递归求深度"></a>2.递归求深度</h3><p>+++</p> <img src="https://pic.leetcode-cn.com/f26ceb47110a4d431260a6af03c127027d9e1bdfd3980c553376734ca29fe545-image.png" alt="image.png" style="zoom: 67%;" /> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxDepth</span><span class="params">(TreeNode root)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> root==<span class="literal">null</span>? <span class="number">0</span> : Math.max(maxDepth(root.left), maxDepth(root.right))+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="3-BFS"><a href="#3-BFS" class="headerlink" title="3.BFS"></a>3.BFS</h3><p>+++</p><p> BFS的实现原理就是一层层遍历，统计一下总共有多少层，我们来画个图分析一下。 </p> <img src="https://pic.leetcode-cn.com/4d908dab4fe456418f3c06a124c4a0391c67f19780bfafc24d33878541faa665-image.png" alt="image.png" style="zoom:67%;" /> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxDepth</span><span class="params">(TreeNode root)</span> &#123;</span><br><span class="line">        Deque&lt;TreeNode&gt; deque = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        deque.push(root);</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (!deque.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">//每一层的个数</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> deque.size();</span><br><span class="line">            <span class="keyword">while</span> (size-- &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">TreeNode</span> <span class="variable">cur</span> <span class="operator">=</span> deque.pop();</span><br><span class="line">                <span class="keyword">if</span> (cur.left != <span class="literal">null</span>)</span><br><span class="line">                    deque.addLast(cur.left);</span><br><span class="line">                <span class="keyword">if</span> (cur.right != <span class="literal">null</span>)</span><br><span class="line">                    deque.addLast(cur.right);</span><br><span class="line">            &#125;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;列表&quot;&gt;&lt;a href=&quot;#列表&quot; class=&quot;headerlink&quot; title=&quot;列表&quot;&gt;&lt;/a&gt;列表&lt;/h1&gt;&lt;h2 id=&quot;创建链表&quot;&gt;&lt;a href=&quot;#创建链表&quot; class=&quot;headerlink&quot; title=&quot;创建链表&quot;&gt;&lt;/a&gt;创建链表&lt;/h</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="算法" scheme="http://example.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop</title>
    <link href="http://example.com/2022/03/06/hadoop/"/>
    <id>http://example.com/2022/03/06/hadoop/</id>
    <published>2022-03-06T10:45:28.926Z</published>
    <updated>2022-03-06T15:16:00.164Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Mapper、Reducer，Driver构造步骤"><a href="#Mapper、Reducer，Driver构造步骤" class="headerlink" title="Mapper、Reducer，Driver构造步骤"></a>Mapper、Reducer，Driver构造步骤</h3><h4 id="Mapper："><a href="#Mapper：" class="headerlink" title="Mapper："></a>Mapper：</h4><p>​    1.extend Mapper&lt;k,v,k,v&gt;,注意出入和输出kv的格式（<font color='red'>与java不同注意包</font>）  </p><p>​    2.然后重写 map方法，根据源码可知 里面的map方法有几个kv就运行几次所以要把一些创建类的东西放在方法外面。</p><p>​    3.context.write(k,v);，提交时要注意kv的数据类型。</p><h4 id="Reducer："><a href="#Reducer：" class="headerlink" title="Reducer："></a>Reducer：</h4><p>​    1.extend Reducer&lt;kv,v,k,v&gt;,注意出入和输出kv的格式（与java不同注意包）  </p><p>​    2.然后重写 reduce方法，根据源码可知 里面的reduce方法会根据value的个数来运行几次所以要把一些创建类的东西放在方法外面。</p><p>​    3.context.write(k,v);，提交时要注意kv的数据类型。</p><h4 id="Driver："><a href="#Driver：" class="headerlink" title="Driver："></a>Driver：</h4><p>​    1.获取Job对象</p><p>​    2.关联Driver类</p><p>​    3.关联Mapper类与关联Reducer类</p><p>​    4.设置Map的KV类型</p><p>​    5.设置最终输出的KV类型</p><p>​    6.设置程序的输入输出文件路径</p><p>​    7.提交Job</p><p><strong>MapTask并行度决定机制</strong></p><p><strong>数据块：</strong>Block是HDFS物理上把数据分成一块一块。数据块是HDFS存储数据单位。</p><p><strong>数据切片：</strong>数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask。</p><img src="/2022/03/06/hadoop/1646563177298.png" class="" width="1646563177298"><p>​    如果分为100M的话 DataNode2还要从 1 中获取 28M与自己的100M进行合并运算，中间有传输时间和合并时间，效率低，所以要根据块的大小来设置切片大小。</p><h3 id="Job提交流程源码详解"><a href="#Job提交流程源码详解" class="headerlink" title="Job提交流程源码详解"></a><strong>Job提交流程源码详解</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">// 1建立连接</span><br><span class="line"></span><br><span class="line">​connect();</span><br><span class="line"></span><br><span class="line">​// 1）创建提交Job的代理</span><br><span class="line"></span><br><span class="line">​new Cluster(getConfiguration());</span><br><span class="line"></span><br><span class="line">​// （1）判断是本地运行环境还是yarn集群运行环境</span><br><span class="line"></span><br><span class="line">​initialize(jobTrackAddr, conf);   //返回0是本地  1是yarn</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">// 2 提交job</span><br><span class="line"></span><br><span class="line">submitter.submitJobInternal(Job.this, cluster)</span><br><span class="line"></span><br><span class="line"> 首先检察输出路径是否存在（会抛出异常 1.为空（没有填写输出路径），2.路径存在）</span><br><span class="line"></span><br><span class="line">​// 1）创建给集群提交数据的Stag路径（临时缓存路径）</span><br><span class="line"></span><br><span class="line">​Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​// 2）获取jobId（每一个job都有一个id） ，并创建Job路径  在上一步的Stag路径后加上jobID</span><br><span class="line"></span><br><span class="line">​JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​// 3）拷贝jar包到集群</span><br><span class="line"></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir); //本地模式不会提交jar  集群模式会提交</span><br><span class="line"></span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​// 4）计算切片，生成切片规划文件</span><br><span class="line"></span><br><span class="line">​writeSplits(job, submitJobDir);</span><br><span class="line"></span><br><span class="line">​maps = writeNewSplits(job, jobSubmitDir);   //根据切片信息生产几个mapTask</span><br><span class="line"></span><br><span class="line">​input.getSplits(job);      //把切片信息保存到 jobID路径</span><br><span class="line">根基切片个数生成几个map   并把切片信息以log方式输出，在执行日志中也可以看见</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​// 5）向Stag路径写XML配置文件</span><br><span class="line"></span><br><span class="line">writeConf(conf, submitJobFile);    //任然保存到JobID下的路径 </span><br><span class="line"></span><br><span class="line">​conf.writeXml(out); </span><br><span class="line"></span><br><span class="line">​// 6）提交Job,返回提交状态</span><br><span class="line"></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">//提交完毕后会将刚才生产的缓存文件删除</span><br></pre></td></tr></table></figure><p>最终提交信息为：</p><img src="/2022/03/06/hadoop/1646528945437.png" class="" width="1646528945437"><img src="/2022/03/06/hadoop/1646471050412.png" class="" width="1646471050412"><img src="/2022/03/06/hadoop/1646563186420.png" class="" width="1646563186420"><img src="/2022/03/06/hadoop/1646563193170.png" class="" width="1646563193170"><p>minSize &#x3D; 1; maxSize &#x3D; long的最大值    bolckSize &#x3D; 128m  （在本地运行为32m）</p><p><strong>TextInputFormat</strong></p><p>​        TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型。</p><p><img src="/1646470491592.png" alt="1646470491592"></p><h3 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a><strong>CombineTextInputFormat</strong>切片机制</h3><p>​    CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p><p><strong>虚拟存储切片最大值设置：</strong><font color='cornflowerblue'>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);&#x2F;&#x2F; 4m</font></p><p><font color='orange'>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</font></p><p><strong>切片机制：</strong></p><p>​    生成切片过程包括：虚拟存储过程和切片过程二部分。</p><img src="/2022/03/06/hadoop/1646563201262.png" class="" width="1646563201262"><h4 id="虚拟存储过程："><a href="#虚拟存储过程：" class="headerlink" title="虚拟存储过程："></a>虚拟存储过程：</h4><p>​    将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。</p><p>​    例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。</p><p><strong>例：</strong></p><p>​    输入数据为4个小文件，在不经过任何处理运行时切片个数为4个，当在Driver中增加<code>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); </code>时，切片个数为为3个</p><p>当将大小改为 20971520（20m）时切片各个变为1个。只产生一个MapTask提高了效率。</p><h3 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h3><img src="/2022/03/06/hadoop/1646563207533.png" class="" width="1646563207533"><img src="/2022/03/06/hadoop/1646563211839.png" class="" width="1646563211839"><p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：</p><p>（1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</p><p>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p><p>（3）多个溢出文件会被合并成大的溢出文件</p><p>（4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</p><p>（5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</p><p>（6）ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</p><p>（7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</p><p><strong>注意：</strong></p><p>（1）Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</p><p>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。</p><p><strong>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffer</strong></p><img src="/2022/03/06/hadoop/1646563217632.png" class="" width="1646563217632"><h3 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h3><p>​    默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p><p><img src="/1646472095363.png" alt="1646472095363"></p><p>!<img src="/1646472279827.png" alt="1646472279827">(1646472264084.png)</p><p><strong>KV设置为map输出的KV</strong></p><h4 id="分区总结"><a href="#分区总结" class="headerlink" title="分区总结"></a>分区总结</h4><ol><li><p>如果RedhceTask的数量 &gt; getPantition的结果数，则会多产生几个空的输出文件part-r-000xx;</p></li><li><p>如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会Exception;</p></li><li><p>如果ReduceTask的数量 &#x3D; 1，<strong>不执行分区操作，在执行之前首先要判断ReduceTas数量是否为1</strong>最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件part-r-00000;</p></li><li><p>分区号必须从零开始，逐一累加。</p><p><img src="/1646472445540.png" alt="1646472445540"></p></li></ol><h3 id="WritableComparable排序"><a href="#WritableComparable排序" class="headerlink" title="WritableComparable排序"></a>WritableComparable排序</h3><img src="/2022/03/06/hadoop/1646563228969.png" class="" width="1646563228969"><ol><li>部分排序<br>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。</li><li>全排序<br>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</li><li>辅助排序:(GroupingComparator分组)<br>在Reduce端对key进行分组。应用于:在接收的key为bean对象时，想让一个或几个字段相同（全部<br>字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序。</li><li>二次排序<br>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</li></ol><h3 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a><strong>Combiner合并</strong></h3><ol><li><p>Combiner是MR程序中Mapper和Reducer之外的一种组件。</p></li><li><p>Combiner组件的父类就是Redicer。</p></li><li><p>Combiner和Reducen的区别在于运行的位置Combiner是在每一个MapTask所在的节点运行;Reducer是接收全局所有Mappen的输出结果;</p><p>Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</p></li><li><p>Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</p></li></ol><p><strong>！！！求平均值的情况不能用Combiner 会导致结果不正确。</strong></p><p>求和时可以利用Combiner</p><p><em><strong>Combiner主要是作为一个小型的Redicer在每一个MapTask上进行求和计算，从而减少数据传输导致的时间过长</strong></em></p><h3 id="OutputFormat数据输出"><a href="#OutputFormat数据输出" class="headerlink" title="OutputFormat数据输出"></a>OutputFormat数据输出</h3><p>   默认输出格式为TextOutputFormat   一行一行写到text文件中</p><h5 id="自定义OutputFormat类"><a href="#自定义OutputFormat类" class="headerlink" title="自定义OutputFormat类"></a>自定义OutputFormat类</h5><ul><li>自定义一个类继承FileOutputFormat。</li><li>改RecordWriter，具体改写方法write（）</li></ul><p><strong>方法：</strong></p><ul><li>​    自定义MyFileOutputFormat类后重写getRecordWriter()函数，设置返回值的类型为KV,向自定义MYRecordWriter类传递job。</li><li>​    在构造函数中获取传来的job，定义输出流（写出路径），重写writer&lt;K,V&gt;方法，根据条件选择输出流。</li><li>​    在close（）方法中关闭创建的输出流。</li></ul><h3 id="MapReduce内核源码解析"><a href="#MapReduce内核源码解析" class="headerlink" title="MapReduce内核源码解析"></a>MapReduce内核源码解析</h3><h4 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h4><img src="/2022/03/06/hadoop/1646563236218.png" class="" width="1646563236218"><ul><li><font color='red'>Read阶段</font>：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key&#x2F;value。</li><li><font color='red'>Map阶段</font>：该节点主要是将解析出的key&#x2F;value交给用户编写map()函数处理，并产生一系列新的key&#x2F;value。</li><li><font color='red'>Collect收集阶段</font>：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key&#x2F;value分区（调用Partitioner），并写入一个环形内存缓冲区中。</li><li><font color='red'>Spill阶段</font>：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</li></ul><p>溢写阶段详情：</p><p>​    步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output&#x2F;spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>​    步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output&#x2F;spillN.out.index中。</p><ul><li><p><font color='red'>Merge阶段</font>：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output&#x2F;file.out中，同时生成相应的索引文件output&#x2F;file.out.index。</p><p>​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>​    让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p></li></ul><h4 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a><strong>ReduceTask工作机制</strong></h4><p><img src="/1646529956381.png" alt="img"></p><ol><li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</li><li>Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</li><li>Reduce阶段：reduce()函数将计算结果写到HDFS上。</li></ol><h4 id="ReduceTask并行度决定机制"><a href="#ReduceTask并行度决定机制" class="headerlink" title="ReduceTask并行度决定机制"></a>ReduceTask并行度决定机制</h4><p>****回顾：****MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p><p>****思考：****ReduceTask并行度由谁决定？</p><p>ReduceTask数量的决定是可以直接手动设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 默认值是1，手动设置为4</span><br><span class="line">job.setNumReduceTasks(4);</span><br></pre></td></tr></table></figure><img src="/2022/03/06/hadoop/1646563282412.png" class="" width="1646563282412"><h3 id="Join-应用"><a href="#Join-应用" class="headerlink" title="Join 应用"></a><strong>Join</strong> <strong>应用</strong></h3><h4 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a><strong>Reduce Join</strong></h4><p>​    Map 端的主要工作：为来自不同表或文件的 key&#x2F;value 对，打标签以区别不同来源的记 </p><p>录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。 </p><p>​     Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要 </p><p>在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进 </p><p>行合并.</p><h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a><strong>Map Join</strong></h4><p><strong>1</strong>）使用场景</p><p>Map Join 适用于一张表十分小、一张表很大的场景。 </p><p><strong>2</strong>）优点</p><p>思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？ </p><p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数 </p><p>据的压力，尽可能的减少数据倾斜。 </p><p><strong>3</strong>）具体办法：采用 <strong>DistributedCache</strong> </p><p>（1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。 </p><p>（2）在 Driver 驱动类中加载缓存。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//缓存普通文件到 Task 运行节点。</span><br><span class="line">job.addCacheFile(new URI(&quot;file:///e:/cache/pd.txt&quot;));</span><br><span class="line">//如果是集群运行,需要设置 HDFS 路径</span><br><span class="line">job.addCacheFile(new URI(&quot;hdfs://hadoop102:8020/cache/pd.txt&quot;));</span><br></pre></td></tr></table></figure><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>数据清洗往往在map阶段就实现了，不需要reduce阶段 ，如果只需要数据清洗可以将reducetask设置为0个</p><h3 id="MapReduce-开发总结"><a href="#MapReduce-开发总结" class="headerlink" title="MapReduce 开发总结"></a><strong>MapReduce</strong> <strong>开发总结</strong></h3><p>1.输入数据接口：<strong>InputFormat</strong> </p><p>​    （1）默认使用的实现类是：TextInputFormat </p><p>​    （2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。 </p><p>​    （3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。 </p><p>2.逻辑处理接口：<strong>Mapper</strong>  </p><p>​    用户根据业务需求实现其中三个方法： setup() map() cleanup ()  </p><p><strong>3.Partitioner</strong> <strong>分区</strong> </p><p>​    （1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces </p><p>​    （2）如果业务上有特别的需求，可以自定义分区。 </p><p><strong>4.Comparable</strong> <strong>排序</strong></p><p>​    （1）当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。 </p><p>​    （2）部分排序：对最终输出的每一个文件进行内部排序。 </p><p>​    （3）全排序：对所有数据进行排序，通常只有一个 Reduce。 </p><p>​    （4）二次排序：排序的条件有两个。 </p><p><strong>5.Combiner</strong> <strong>合并</strong> </p><p>​    Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。 </p><p>6.逻辑处理接口：<strong>Reducer</strong> </p><p>​    用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()  </p><p>7.输出数据接口：<strong>OutputFormat</strong> </p><p>​    （1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。 </p><p>​    （2）用户还可以自定义 OutputFormat。 </p><h3 id="Hadoop-数据压缩"><a href="#Hadoop-数据压缩" class="headerlink" title="Hadoop 数据压缩"></a><strong>Hadoop</strong> <strong>数据压缩</strong></h3><h4 id="压缩的好处和坏处："><a href="#压缩的好处和坏处：" class="headerlink" title="压缩的好处和坏处："></a><strong>压缩的好处和坏处：</strong></h4><p>​    压缩的优点：以减少磁盘 IO、减少磁盘存储空间。 </p><p>​    压缩的缺点：增加 CPU 开销。</p><h4 id="压缩原则："><a href="#压缩原则：" class="headerlink" title="压缩原则："></a><strong>压缩原则：</strong></h4><p>​    （1）运算密集型的 Job，少用压缩 </p><p>​    （2）IO 密集型的 Job，多用压缩 </p><h4 id="压缩方式选："><a href="#压缩方式选：" class="headerlink" title="压缩方式选："></a><strong>压缩方式选：</strong></h4><p>​    压缩方式选择时重点考虑：<strong>压缩&#x2F;解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片</strong>。</p><h4 id="压缩位置选择："><a href="#压缩位置选择：" class="headerlink" title="压缩位置选择："></a><strong>压缩位置选择：</strong></h4><p>​    压缩可以在 MapReduce 作用的任意阶段启用。 </p><img src="/2022/03/06/hadoop/1646563292818.png" class="" width="1646563292818"><table><thead><tr><th>压缩格式</th><th>对应的编码&#x2F;解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>无，这个需要在命令行输入hadoop checknative查看</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr></tbody></table><h4 id="压缩实例"><a href="#压缩实例" class="headerlink" title="压缩实例"></a>压缩实例</h4><p>都在Driver里进行设置</p><p>​    在map阶段压缩要在 conf创建后进行设置</p><p>​    在reduce阶段压缩要在设置输入输出路径后设置</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Mapper、Reducer，Driver构造步骤&quot;&gt;&lt;a href=&quot;#Mapper、Reducer，Driver构造步骤&quot; class=&quot;headerlink&quot; title=&quot;Mapper、Reducer，Driver构造步骤&quot;&gt;&lt;/a&gt;Mapper、Redu</summary>
      
    
    
    
    <category term="Hadoop" scheme="http://example.com/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 生产调优手册</title>
    <link href="http://example.com/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/"/>
    <id>http://example.com/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/</id>
    <published>2022-03-06T10:45:27.767Z</published>
    <updated>2022-03-06T15:17:32.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HDFS—核心参数"><a href="#HDFS—核心参数" class="headerlink" title="HDFS—核心参数"></a>HDFS—核心参数</h2><h3 id="NameNode-内存生产配置"><a href="#NameNode-内存生产配置" class="headerlink" title="NameNode 内存生产配置"></a>NameNode 内存生产配置</h3><p><strong>1）NameNode 内存计算</strong><br>    每个文件块大概占用 150byte，一台服务器 128G 内存为例，能存储多少文件块呢？<br>128 * 1024 * 1024 * 1024 &#x2F; 150Byte ≈ 9.1 亿</p><p><strong>2）Hadoop2.x 系列，配置 NameNode 内存</strong><br>    NameNode 内存默认 2000m，如果服务器内存 4G，NameNode 内存可以配置 3g。在<br>hadoop-env.sh 文件中配置如下。</p><p><code>HADOOP_NAMENODE_OPTS=-Xmx3072m</code> </p><p><strong>3）Hadoop3.x 系列，配置 NameNode 内存</strong> </p><p>（1）hadoop-env.sh 中描述 Hadoop 的内存是动态分配的</p><p>（2）查看 NameNode 占用内存</p><p>[atguigu@hadoop102 ~]  jps<br>3088 NodeManager<br>2611 NameNode<br>3271 JobHistoryServer<br>2744 DataNode<br>3579 Jps<br>[atguigu@hadoop102 ~]​  jmap -heap 2611<br>  Heap Configuration:<br> MaxHeapSize      &#x3D;           1031798784 (984.0MB)</p><p>查看发现 hadoop102 上的 NameNode 和 DataNode 占用内存都是自动分配的，且相等。 </p><p>不是很合理。<img src="/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/1646549131508-1646562909757.png" class="" width="1646549131508"></p><p>具体修改：hadoop-env.sh</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -</span><br><span class="line">Xmx1024m&quot;</span><br><span class="line">export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS</span><br><span class="line">-Xmx1024m&quot;</span><br></pre></td></tr></table></figure><h3 id="NameNode-心跳并发配置"><a href="#NameNode-心跳并发配置" class="headerlink" title="** NameNode** 心跳并发配置"></a>** NameNode** <strong>心跳并发配置</strong></h3><p>NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发<br>的元数据操作。<br>对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是 10。<br><property><br> <name>dfs.namenode.handler.count</name><br> <value>21</value><br></property></p><p>企业经验：dfs.namenode.handler.count&#x3D;20 × 𝑙𝑜𝑔𝑒 𝐶𝑙𝑢𝑠𝑡𝑒𝑟 𝑆𝑖𝑧𝑒，比如集群规模（DataNode 台 </p><p>数）为 3 台时，此参数设置为 21。可通过简单的 python 代码计算该值，代码如下。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo yum install -y python</span><br><span class="line">[atguigu@hadoop102 ~]$ python</span><br><span class="line">Python 2.7.5 (default, Apr 11 2018, 07:36:10) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more </span><br><span class="line">information.</span><br><span class="line">&gt;&gt;&gt; import math</span><br><span class="line">&gt;&gt;&gt; print int(20*math.log(3))</span><br><span class="line">21</span><br><span class="line">&gt;&gt;&gt; quit()</span><br></pre></td></tr></table></figure><h3 id="开启回收站配置"><a href="#开启回收站配置" class="headerlink" title="开启回收站配置"></a><strong>开启回收站配置</strong></h3><p>​    开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、 </p><p>备份等作用。 </p><p><strong>1）回收站工作机制</strong></p>回收站功能参数说明**<p>（1）默认值 fs.trash.interval &#x3D; 0，0 表示禁用回收站；其他值表示设置文件的存活时间。 </p><p>（2）默认值 fs.trash.checkpoint.interval &#x3D; 0，检查回收站的间隔时间。如果该值为 0，则该 </p><p>值设置和 fs.trash.interval 的参数值相等。 </p><p>（3）要求 fs.trash.checkpoint.interval &lt;&#x3D; fs.trash.interval.</p><p><strong>3）启用回收站</strong></p><p>修改 core-site.xml，配置垃圾回收时间为 1 分钟。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>4）</strong>查看回收站 </p><p>​    回收站目录在 HDFS 集群中的路径：&#x2F;user&#x2F;atguigu&#x2F;.Trash&#x2F;…. </p><p><strong>5）</strong>注意：通过网页上直接删除的文件也不会走回收站。 </p><p><strong>6）</strong>通过程序删除的文件不会经过回收站，需要调用 moveToTrash()才进入回收站</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Trash trash = New Trash(conf);</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure><p><strong>7）</strong>只有在命令行利用 hadoop fs -rm 命令删除的文件才会走回收站。</p><p><strong>8）</strong>恢复回收站数据，是找到回收路径把他copy或移动出来</p><h2 id="HDFS—集群压测"><a href="#HDFS—集群压测" class="headerlink" title="HDFS—集群压测"></a>HDFS—集群压测</h2><p>在企业中非常关心每天从 Java 后台拉取过来的数据，需要多久能上传到集群？消费者 关心多久能从 HDFS 上拉取需要的数据？ </p><p>为了搞清楚 HDFS 的读写性能，生产环境上非常需要对集群进行压测。</p><p>➢ <strong>Number of files：</strong>生成 mapTask 数量，一般是集群中（CPU 核数-1），我们测试虚 拟机就按照实际的物理内存-1 分配即可 </p><p>➢ T<strong>otal MBytes processed****：</strong>单个 map 处理的文件大小 </p><p>➢ **Throughput mb&#x2F;sec:**单个 mapTak 的吞吐量 </p><p>计算方式：处理的总文件大小&#x2F;每一个 mapTask 写数据的时间累加 </p><p>集群整体吞吐量：生成 mapTask 数量*单个 mapTak 的吞吐量 </p><p>➢ <strong>Average IO rate mb&#x2F;sec:</strong> 平均 mapTak 的吞吐量 </p><p>计算方式：每个 mapTask 处理文件大小&#x2F;每一个 mapTask 写数据的时间全部相加除以 task 数量 </p><p>➢ <strong>IO rate std deviation:</strong> 方差、反映各个 mapTask 处理的差值，越小越均衡</p><p><strong>1）测试内容：向 HDFS 集群写 10 个 128M 的文件</strong></p><p><img src="C:\Users\26959\AppData\Roaming\Typora\typora-user-images\1646549566406.png" alt="1646549566406"></p><p><strong>2）注意：如果测试过程中，出现异常</strong> </p><p>(1）可以在 yarn-site.xml 中设置虚拟内存检测为 false</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则</span><br><span class="line">直接将其杀掉，默认是 true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>(2）分发配置并重启 Yarn 集群</p><p><strong>3）测试结果分析</strong></p><p>（1）由于副本 1 就在本地，所以该副本不参与测试 </p><p>一共参与测试的文件：10 个文件 * 2 个副本 &#x3D; 20 个 </p><p>压测后的速度：1.61 </p><p>实测速度：1.61M&#x2F;s * 20 个文件 ≈ 32M&#x2F;s </p><p>三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30m&#x2F;s </p><p>所有网络资源都已经用满。 </p><p><strong>如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘</strong> </p><p><strong>或者增加磁盘个数。</strong> </p><p>（2）如果客户端不在集群节点，那就三个副本都参与计算</p><h2 id="HDFS—多目录"><a href="#HDFS—多目录" class="headerlink" title="HDFS—多目录"></a>HDFS—多目录</h2><p>1）NameNode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p><p>2）具体配置如下</p><p>​        1.在 hdfs-site.xml 文件中添加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line"> </span><br><span class="line">&lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.</span><br><span class="line">dir&#125;/dfs/name2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>​        2.停止集群，删除三台节点的 data 和 logs 中所有数据.</p><p>​        3.格式化集群并启动。 </p><p>3)DataNode的多目录配置一样，只不过不需要删除数据和格式化</p><h3 id="集群数据均衡之磁盘间数据均衡"><a href="#集群数据均衡之磁盘间数据均衡" class="headerlink" title="集群数据均衡之磁盘间数据均衡"></a><strong>集群数据均衡之磁盘间数据均衡</strong></h3><p>生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可<br>以执行磁盘数据均衡命令。（Hadoop3.x 新特性）</p><p>（1）生成均衡计划（<strong>我们只有一块磁盘，不会生成计划</strong>） </p><p><code>hdfs diskbalancer -plan hadoop103</code> </p><p>（2）执行均衡计划 </p><p><code>hdfs diskbalancer -execute hadoop103.plan.json</code> </p><p>（3）查看当前均衡任务的执行情况 </p><p><code>hdfs diskbalancer -query hadoop103</code> </p><p>（4）取消均衡任务 </p><p><code>hdfs diskbalancer -cancel hadoop103.plan.json</code> </p><h2 id="HDFS—集群扩容及缩容"><a href="#HDFS—集群扩容及缩容" class="headerlink" title="HDFS—集群扩容及缩容"></a>HDFS—集群扩容及缩容</h2><h3 id="添加白名单"><a href="#添加白名单" class="headerlink" title="添加白名单"></a><strong>添加白名单</strong></h3><p>白名单：表示在白名单的主机 IP 地址可以，用来存储数据。 </p><p>企业中：配置白名单，可以尽量防止黑客恶意访问攻击。 </p><p>配置白名单步骤如下： </p><p>1）在 NameNode 节点的&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop 目录下分别创建 whitelist 和 </p><p>blacklist 文件 </p><p>（1）创建白名单 </p><p><code>[atguigu@hadoop102 hadoop]$ vim whitelist</code> </p><p>在 whitelist 中添加如下主机名称，假如集群正常工作的节点为 102 103 </p><p>（2）创建黑名单 </p><p><code>[atguigu@hadoop102 hadoop]$ touch blacklist</code> </p><p>保持空的就可以</p><p>2）在 hdfs-site.xml 配置文件中增加 dfs.hosts 配置参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 白名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.hosts&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）分发配置文件 whitelist，hdfs-site.xml </p><p><code>[atguigu@hadoop104 hadoop]$ xsync hdfs-site.xml whitelist</code> </p><p>4）第一次添加白名单必须重启集群，不是第一次，只需要刷新 NameNode 节点即可 </p><p><code>[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh stop</code> </p><p><code>[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</code> </p><h3 id="服役新服务器"><a href="#服役新服务器" class="headerlink" title="服役新服务器"></a><strong>服役新服务器</strong></h3><p>1）需求 </p><p>​    随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据 </p><p>的需求，需要在原有集群基础上动态添加新的数据节点.</p><p>2）环境准备 </p><p>（1）在 hadoop100 主机上再克隆一台 hadoop105 主机 </p><p>（2）修改 IP 地址和主机名称</p><p>（3）拷贝 hadoop102 的&#x2F;opt&#x2F;module 目录和&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 到 hadoop105</p><p>（4）删除 hadoop105 上 Hadoop 的历史数据，data 和 log 数据 </p><p>（5）配置 hadoop102 和 hadoop103 到 hadoop105 的 ssh 无密登录 </p><p>3）服役新节点具体步骤 </p><p>（1）直接启动 DataNode，即可关联到集群</p><p>4）在白名单中增加新服役的服务器</p><h3 id="服务器间数据均衡"><a href="#服务器间数据均衡" class="headerlink" title="服务器间数据均衡"></a><strong>服务器间数据均衡</strong></h3><p>1）企业经验： </p><p>在企业开发中，如果经常在 hadoop102 和 hadoop104 上提交任务，且副本数为 2，由于数据本地性原则，就会导致 hadoop102 和 hadoop104 数据过多，hadoop103 存储的数据量小。另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。 </p><p>2）开启数据均衡命令： </p><p><code>[atguigu@hadoop105 hadoop-3.1.3]$ sbin/start-balancer.sh - threshold 10</code> </p><p>对于参数 10，代表的是集群中各个节点的磁盘空间利用率相差不超过 10%，可根据实 </p><p>际情况进行调整。</p><p>3）停止数据均衡命令： </p><p><code>[atguigu@hadoop105 hadoop-3.1.3]$ sbin/stop-balancer.sh</code> </p><p>注意：由于 HDFS 需要启动单独的 Rebalance Server 来执行 Rebalance 操作，所以尽量 </p><p>不要在 NameNode 上执行 start-balancer.sh，而是找一台比较空闲的机器。</p><h3 id="黑名单退役服务器"><a href="#黑名单退役服务器" class="headerlink" title="黑名单退役服务器"></a><strong>黑名单退役服务器</strong></h3><p>黑名单：表示在黑名单的主机 IP 地址不可以，用来存储数据。 </p><p>企业中：配置黑名单，用来退役服务器。 </p><p>黑名单配置步骤如下： </p><p>1）编辑&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop 目录下的 blacklist 文件</p><p>​    添加如下主机名称（要退役的节点）    </p><p>2）分发配置文件 blacklist，hdfs-site.xml</p><p>3）第一次添加黑名单必须重启集群，不是第一次，只需要刷新 NameNode 节点即可</p><p>4）检查 Web 浏览器，退役节点的状态为 decommission in progress（退役中），说明数据 </p><p>节点正在复制块到其他节点 </p><img src="/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/1646553722796.png" class="" width="1646553722796"><p>5<strong>）等待退役节点状态为</strong> decommissioned<strong>（所有块已经复制完成），停止该节点及节点资源</strong> </p><p>管理器。注意：如果副本数是 3<strong>，服役的节点小于等于</strong> 3<strong>，是不能退役成功的，需要修改</strong> </p><p>副本数后才能退役</p><img src="/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/1646553737485.png" class="" width="1646553737485"><p>6）如果数据不均衡，可以用命令实现集群的再平衡</p><h2 id="HDFS—存储优化"><a href="#HDFS—存储优化" class="headerlink" title="HDFS—存储优化"></a>HDFS—存储优化</h2><h3 id="纠删码原理"><a href="#纠删码原理" class="headerlink" title="纠删码原理"></a><strong>纠删码原理</strong></h3><p>​    HDFS 默认情况下，一个文件有 3 个副本，这样提高了数据的可靠性，但也带来了 2 倍的冗余开销。Hadoop3.x 引入了纠删码，采用计算的方式，可以节省约 50％左右的存储空间</p><img src="/2022/03/06/Hadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89/1646553806181.png" class="" width="1646553806181">]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;HDFS—核心参数&quot;&gt;&lt;a href=&quot;#HDFS—核心参数&quot; class=&quot;headerlink&quot; title=&quot;HDFS—核心参数&quot;&gt;&lt;/a&gt;HDFS—核心参数&lt;/h2&gt;&lt;h3 id=&quot;NameNode-内存生产配置&quot;&gt;&lt;a href=&quot;#NameNode-内</summary>
      
    
    
    
    <category term="Hadoop" scheme="http://example.com/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
</feed>
